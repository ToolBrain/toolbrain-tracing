# ToolBrain Tracing: The Open-Source AI Operations Center (AIOC) ğŸ§ ğŸš€

**ToolBrain Tracing** is a comprehensive **AI Operations Center (AIOC)** designed for the era of Agentic AI. 

In a world where AI agents are increasingly autonomous, ToolBrain provides the necessary infrastructure to **observe**, **govern**, and **evolve** agentic workflows. It moves beyond passive logging to create a dynamic, closed-loop system where agents learn from past experiences and humans maintain real-time control.

## âœ¨ Key Features

### ğŸ“¥ Ingestion Layer (Standardization)
- **Unified Observability**: Capture agent workflows in a standardized OTLP format.
- **Framework-Agnostic**: Simple SDK/CLI to integrate any agent (LangChain, SmolAgents, or custom).
- **Delta-based Tracing**: Highly storage-efficient prompt tracking via `new_content` logic.

### ğŸ§  Cognitive Layer (Self-Evolving)
- **Experience Retrieval**: `search_past_experiences` tool for agents to perform Dynamic ICL using past successes.
- **Curriculum Automation**: Automatically generate and export training tasks from failed traces.
- **Semantic Search**: Powered by `pgvector` for finding similar reasoning patterns.

### ğŸ›¡ï¸ Governance Layer (Human-in-the-loop)
- **Active Help Request**: Real-time escalation tool for agents to signal uncertainty to human experts.
- **Interactive Command Center**: Visualize complex multi-turn traces and provide expert feedback.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your AI Agent  â”‚â”€â”€â”€â”€â”€â–¶â”‚  TraceStore API  â”‚â”€â”€â”€â”€â”€â–¶â”‚  PostgreSQL â”‚
â”‚   (smolagents,  â”‚      â”‚    (FastAPI)     â”‚      â”‚   / SQLite  â”‚
â”‚    or custom)   â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                 â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Admin Panel UI  â”‚
                       â”‚                  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Your AI Agent:** Any agent framework. Uses the TraceClient SDK to send data.
- **TraceStore API:** The central FastAPI server. Ingests, stores, and serves trace data.
- **Database:** The persistence layer (PostgreSQL or SQLite).
- **Admin Panel UI:** A client (Streamlit/React) that consumes the TraceStore API.

**Tech Stack:**
- **Backend**: FastAPI, SQLAlchemy 2.0, Pydantic V2
- **Database**: PostgreSQL (production), SQLite (development), pgvector (semantic search)
- **Frontend**: Streamlit (Admin UI), React placeholder in `web/`
- **Deployment**: Docker Compose
- **AI Integration**: LibrarianAgent + AI Judge with multi-provider LLM support
- **Embeddings**: sentence-transformers (local) or OpenAI/Gemini (cloud)

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.8+ (for local development)
- PostgreSQL 15+ (if running without Docker)

### Option 1: Docker (Recommended)

1. **Clone the repository**
   ```bash
   git clone https://github.com/ToolBrain/toolbrain-tracing.git
   cd toolbrain-tracing
   ```

2. **Start the services**
   ```bash
   # Install the CLI tool
   pip install -e .
   
   # Start PostgreSQL + API server
   toolbrain-trace up
   ```

3. **Access the services**
   - API: http://localhost:8000
   - API Docs: http://localhost:8000/docs
   - Streamlit UI: Run separately with `streamlit run src/examples/app.py`
   - If needed: `pip install streamlit pandas`

4. **Seed sample data** (optional)
   ```bash
   cd src/examples
   python seed_tracestore_samples.py --backend postgresql --db-url "postgresql://traceuser:tracepass@localhost:5432/tracedb"
   ```

### Option 2: Local Development

1. **Create and activate a virtual environment**
    ```bash
    python -m venv .venv
    # Windows (PowerShell)
    .\.venv\Scripts\Activate.ps1
    # macOS/Linux
    source .venv/bin/activate
    ```

2. **Install dependencies**
    ```bash
    pip install -e .
    # Optional local embeddings
    # pip install -e .[embeddings-local]
    ```

3. **Run the API server** (SQLite mode)
    ```bash
    toolbrain-trace init-db
    toolbrain-trace start
    ```

4. **Run the Streamlit UI**
   ```bash
   cd src/examples
   streamlit run app.py
   ```

## ğŸ“– Usage

### CLI Commands

```bash
# Start Docker services
toolbrain-trace up

# Start with rebuild (after code changes)
toolbrain-trace up --build

# Stop services
toolbrain-trace down

# Manual Docker rebuild (if changes aren't picked up)
docker compose -f docker/docker-compose.yml build --no-cache
```

### API Endpoints

## Concepts

- **Trace**: A single execution attempt (an "experiment").
- **Episode**: A logical group of traces (attempts) aimed at solving a single user task.

**Traces**
- `POST /api/v1/traces` - Create a new trace
- `GET /api/v1/traces` - List all traces
- `GET /api/v1/traces/{trace_id}` - Get trace details
- `POST /api/v1/traces/{trace_id}/feedback` - Add feedback to a trace

**Episodes**
- `GET /api/v1/episodes/{episode_id}` - Get all traces in an episode

**Analytics**
- `GET /api/v1/stats` - Get overall statistics
- `GET /api/v1/analytics/tool_usage` - Get tool usage analytics

**Natural Language Queries**
- `POST /api/v1/natural_language_query` - Query traces with natural language
    - Requires LLM configuration via env vars (for example: `LLM_PROVIDER`, `LLM_API_KEY`)
    - Supports `session_id` for chat memory and returns `suggestions`
- `GET /api/v1/librarian_sessions/{session_id}` - Load stored chat history

**AI Evaluation**
- `POST /api/v1/ai_evaluate/{trace_id}` - Evaluate a trace with a judge model

**Semantic Search**
- `GET /api/v1/traces/search` - Find similar traces using vector similarity

**Governance Signals**
- `POST /api/v1/traces/{trace_id}/signal` - Update trace status/priority

**Curriculum**
- `POST /api/v1/curriculum/generate` - Generate tasks from failed/low-rated traces
- `GET /api/v1/curriculum` - List pending curriculum tasks
- `GET /api/v1/curriculum/export` - Export curriculum tasks as JSONL

**Exports**
- `GET /api/v1/export/traces` - Export raw OTLP traces as JSONL

### Configuration (LLM + Embeddings)

Core environment variables:

```bash
# LLM (Librarian, Judge, Curator)
LLM_PROVIDER=gemini
LLM_MODEL=gemini-2.5-flash
LLM_API_KEY=your-key

# Embeddings (semantic search)
EMBEDDING_PROVIDER=local
EMBEDDING_MODEL=all-MiniLM-L6-v2
```

**Example API Usage:**

```python
import requests

# Create a trace
response = requests.post("http://localhost:8000/api/v1/traces", json={
    "trace_id": "trace-001",
    "spans": [
        {
            "span_id": "span-001",
            "trace_id": "trace-001",
            "name": "User Request",
            "start_time": "2024-01-01T10:00:00Z",
            "end_time": "2024-01-01T10:00:05Z",
            "attributes": {
                "toolbrain.span.type": "user_request",
                "toolbrain.content.new_content": "What's the stock price of NVIDIA?"
            }
        }
    ]
})

# Add feedback
requests.post("http://localhost:8000/api/v1/traces/trace-001/feedback", json={
    "rating": 5,
    "tags": ["accurate", "fast"],
    "comment": "Great response!",
    "metadata": {
        "outcome": "success",
        "efficiency_score": 0.95
    }
})
```

### Streamlit UI

The admin UI provides:
- **Trace Browser**: View all traces with filters
- **Trace Details**: Expandable span tree visualization
- **Feedback Form**: Rate and tag traces
- **Analytics Dashboard**: Stats, tool usage charts
- **AI Librarian**: Session-aware chat with suggestions and history restore
- **AI Evaluation**: Run judge model rating and review history per trace
- **Governance Signal**: Mark traces with status and priority
- **Curriculum**: Generate and review training tasks

Dependencies for the UI:

```bash
pip install streamlit pandas
```

### Embeddings and Semantic Search

Configure embeddings for vector search and experience retrieval:

```bash
# local (default)
EMBEDDING_PROVIDER=local
EMBEDDING_MODEL=all-MiniLM-L6-v2

# cloud (OpenAI/Gemini)
EMBEDDING_PROVIDER=openai
EMBEDDING_API_KEY=your-key
EMBEDDING_MODEL=text-embedding-3-small

# optional for OpenAI-compatible endpoints
EMBEDDING_BASE_URL=https://your-endpoint/v1
```

## ğŸ”Œ Integration with Your Agent

### Using the TraceStore Client

```python
import json

from toolbrain_tracing.sdk.client import TraceClient

client = TraceClient(base_url="http://localhost:8000")

# Submit a trace
client.log_trace({
    "trace_id": "my-trace-001",
    "spans": [...],  # Your OTLP spans
    "feedback": {}
})

# Query traces
traces = client.list_traces()

# Export traces as JSONL
jsonl_payload = client.export_traces(min_rating=4, limit=100)

# Parse JSONL into Python objects
trace_items = [json.loads(line) for line in jsonl_payload.splitlines() if line.strip()]

# Reconstruct messages or turns from OTLP
trace_data = client.get_trace("my-trace-001")
messages = TraceClient.to_messages(trace_data)
turns = TraceClient.to_turns(trace_data)
toolbrain_turns = TraceClient.to_toolbrain_turns(trace_data)
```

### Agent Tools (Experience Retrieval + Active Help Request)

When to use:

- Use `search_past_experiences` to fetch high-quality, previously successful traces for similar tasks.
- Use `search_similar_traces` when you need semantic similarity over trace content.
- Use `request_human_intervention` when the agent is blocked, uncertain, or needs clarification.

```python
from toolbrain_tracing.sdk import (
    search_past_experiences,
    search_similar_traces,
    request_human_intervention,
)

# Retrieve prior successful experiences
experiences = search_past_experiences("resolve a tool error", min_rating=4, limit=3)

# Semantic search over traces
similar = search_similar_traces("multi-step planning", min_rating=4, limit=3)

# Escalate to human when the agent is blocked
help_request = request_human_intervention("User request is ambiguous, need clarification")
```

### Building a Custom Converter

ToolBrain uses the **ToolBrain OTLP (OpenTelemetry Protocol) format** - a delta-based trace schema with parent_id chains for conversation reconstruction.

See [docs/Converter.md](docs/Converter.md) for:
- OTLP schema explanation (parent_id, new_content, delta-based design)
- Step-by-step conversion recipe
- Python template code with examples

**Quick Example:**

```python
import uuid

from toolbrain_tracing.core.schema import ToolBrainAttributes, SpanType

def convert_my_agent_to_otlp(agent_data):
    spans = []
    parent_id = None
    for step in agent_data.steps:
        spans.append({
            "span_id": str(uuid.uuid4()),
            "parent_id": parent_id,  # Chain spans together
            "name": step.action,
            "attributes": {
                ToolBrainAttributes.SPAN_TYPE: SpanType.LLM_INFERENCE,
                ToolBrainAttributes.LLM_NEW_CONTENT: step.output,  # Delta content only
                ToolBrainAttributes.TOOL_NAME: step.tool_name,
            }
        })
        parent_id = spans[-1]["span_id"]
    return {"trace_id": agent_data.id, "spans": spans}
```

## ğŸ“ Project Structure

```
toolbrain-tracing/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ toolbrain_tracing/          # Main package
â”‚   â”‚   â”œâ”€â”€ api/v1/                 # FastAPI REST endpoints
â”‚   â”‚   â”œâ”€â”€ core/                   # TraceStore, schema, agent logic
â”‚   â”‚   â”œâ”€â”€ db/                     # Database session management
â”‚   â”‚   â”œâ”€â”€ sdk/                    # Client SDK
â”‚   â”‚   â”œâ”€â”€ static/                 # Frontend assets
â”‚   â”‚   â”œâ”€â”€ cli.py                  # CLI commands
â”‚   â”‚   â”œâ”€â”€ config.py               # Settings management
â”‚   â”‚   â””â”€â”€ main.py                 # FastAPI app entry
â”‚   â””â”€â”€ examples/                   # Example implementations
â”‚       â”œâ”€â”€ app.py                  # Streamlit admin UI
â”‚       â””â”€â”€ seed_tracestore_samples.py  # Sample data seeder
â”œâ”€â”€ data/                           # Sample OTLP traces
â”‚   â””â”€â”€ ToolBrain OTLP Trace Samples/
â”œâ”€â”€ docker/                         # Docker configuration
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ docs/                           # Documentation
â”‚   â””â”€â”€ Converter.md
â”œâ”€â”€ web/                            # React frontend (future)
â”œâ”€â”€ pyproject.toml                  # Project metadata
â””â”€â”€ README.md
```

## ğŸ› ï¸ Development

### Running Tests

No automated test suite is included yet.

### Seeding Sample Data

```bash
cd src/examples

# SQLite (development)
python seed_tracestore_samples.py --backend sqlite

# PostgreSQL (Docker)
python seed_tracestore_samples.py \
    --backend postgresql \
    --db-url "postgresql://traceuser:tracepass@localhost:5432/tracedb" \
    --samples-dir "../../data/ToolBrain OTLP Trace Samples"
```

### Database Migrations

No migration tooling is included yet. For schema changes:

1. Update models in `src/toolbrain_tracing/db/base.py`
2. Recreate the database:
    - **SQLite (local):** delete `toolbrain_traces.db`, then run `toolbrain-trace init-db`
    - **PostgreSQL (Docker):** `docker compose -f docker/docker-compose.yml down -v` then `toolbrain-trace up`

### Working with JSONB Queries (PostgreSQL)

When querying JSONB fields:

```python
from sqlalchemy import func, cast
from sqlalchemy.dialects.postgresql import JSONB

# Extract text from JSONB
span_type = func.jsonb_extract_path_text(Span.attributes, "toolbrain.span.type")

# Cast for complex queries
rating = func.jsonb_extract_path_text(cast(Trace.feedback, JSONB), "rating")
```

## ğŸ“š Documentation

- **[Building Your Own Trace Converter](docs/Converter.md)** - Complete guide for integrating custom agent frameworks
- **[Trace Reconstruction Guide](docs/Reconstructor.md)** - Rebuild full context from delta traces for training
- **[Sample OTLP Traces](data/ToolBrain%20OTLP%20Trace%20Samples)** - Example trace files
- **[API Documentation](http://localhost:8000/docs)** - Interactive OpenAPI docs (when server is running)
- **[Docker Setup Guide](docker/README.md)** - Docker-specific instructions

## ğŸ¤ Contributing

Contributions are welcome! Here's how to get started:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes and test thoroughly
4. Commit with clear messages: `git commit -m 'Add amazing feature'`
5. Push to your fork: `git push origin feature/amazing-feature`
6. Open a Pull Request

**Development Guidelines:**
- Follow PEP 8 style guide
- Add tests for new features
- Update documentation as needed
- Ensure Docker builds pass

## ğŸ› Troubleshooting

### Docker changes not reflected

If code changes aren't picked up after `toolbrain-trace up --build`:

```bash
toolbrain-trace down
docker compose -f docker/docker-compose.yml build --no-cache
toolbrain-trace up
```

### PostgreSQL connection errors

Ensure PostgreSQL is running and check connection string in `src/toolbrain_tracing/config.py`:

```python
DATABASE_URL = "postgresql://traceuser:tracepass@localhost:5432/tracedb"
```

### Tool usage analytics showing incorrect data

After updating `store.py`, rebuild Docker containers to apply JSONB query fixes.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Built with [FastAPI](https://fastapi.tiangolo.com/)
- Database powered by [SQLAlchemy](https://www.sqlalchemy.org/)
- UI with [Streamlit](https://streamlit.io/)
- Inspired by [OpenTelemetry](https://opentelemetry.io/) standards

---

**Made with â¤ï¸ for the AI agent community**

For questions or support, please open an issue on GitHub.
